  
---  
## ğŸ§¹ Token Filterë€?<br>  
`tokenizer`ì—ì„œ `term` ë¶„ë¦¬ ê³¼ì • ì´í›„ì—ëŠ” ë¶„ë¦¬ëœ ê°ê°ì˜ `term`ë“¤ì´ ì§€ì •ëœ ê·œì¹™ì— ë”°ë¼ì„œ ì²˜ë¦¬ë˜ëŠ”ë° ì´ ì—­í™œì„ `token filter`ê°€ ì§„í–‰í•œë‹¤. `token filter`ëŠ” **filterí•­ëª©ì— ë°°ì—´ë¡œ ì§€ì •**í•´ì•¼ í•˜ê³ , **ì§€ì •í•œ ë°°ì—´ ìˆœì„œëŒ€ë¡œ í•„í„°ê°€ ë™ì‘**í•˜ê¸° ë•Œë¬¸ì— ìˆœì„œë¥¼ ì˜ ê³ ë ¤í•´ì•¼ í•œë‹¤.  
  
ì˜ˆë¥¼ ë“¤ì–´   
```plain text  
"I'm learning Elasticsearch"  
```  
`whitespace` **tokenizer**ë¡œ ë‚˜ëˆ„ë©´  
```json  
["I'm", "learning", "Elasticsearch"]  
```  
ì´ê±¸ `lowercase`, `stop` í† í° í•„í„°ë¥¼ ì ìš©í•˜ë©´  
```json  
["learning", "elasticsearch"]  
```  
  
## âš™ï¸ ì£¼ìš” Token Filter ì¢…ë¥˜<br>  
### 1. `lowercase`<br>  
ëª¨ë“  termë“¤ì„ ì†Œë¬¸ìë¡œ ë°”ê¾¼ë‹¤  
```json  
POST _analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase"],
  "text": "Elasticsearch IS Awesome"
}  
```  
```json  
["elasticsearch", "is", "awesome"]  
```  
  
### 2. `stop`<br>  
ë¶ˆìš©ì–´(**stopwords**)ë¥¼ ì œê±°í•œë‹¤ (ex. `is`, `the`, `a`, `an` â€¦)  
```json  
POST _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop"],
  "text": "this is a test"
}  
```  
```json  
["test"]  
```  
  
### 3. `stemmer`<br>  
ì–´ê·¼(stem)ë§Œ ë‚¨ê¸´ë‹¤. (ex. `running` â†’ `run`)  
```json  
POST _analyze
{
  "tokenizer": "standard",
  "filter": ["stemmer"],
  "text": "running runs runner"
}  
```  
```json  
["run", "run", "runner"]  
```  
  
### 4. `edge_ngram`<br>  
termì„ ì•ë¶€ë¶„ì—ì„œë¶€í„° ìë¥¸ë‹¤. ë³´í†µ ìë™ ì™„ì„±ì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤.  
```json  
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    {
      "type": "edge_ngram",
      "min_gram": 2,
      "max_gram": 4
    }
  ],
  "text": "search"
}  
```  
```json  
["se", "sea", "sear"]  
```  
